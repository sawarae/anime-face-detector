# Anime Face Detector
[![PyPI version](https://badge.fury.io/py/anime-face-detector.svg)](https://pypi.org/project/anime-face-detector/)
[![Downloads](https://pepy.tech/badge/anime-face-detector)](https://pepy.tech/project/anime-face-detector)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hysts/anime-face-detector/blob/main/demo.ipynb)
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange)](https://huggingface.co/spaces/ayousanz/anime-face-detector-gpu)
[![MIT License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)
[![GitHub stars](https://img.shields.io/github/stars/hysts/anime-face-detector.svg?style=flat-square&logo=github&label=Stars&logoColor=white)](https://github.com/hysts/anime-face-detector)
[![Docker Image](https://img.shields.io/badge/Docker-ghcr.io-blue?logo=docker)](https://ghcr.io/ayutaz/anime-face-detector)

This is an anime face detector using
[mmdetection](https://github.com/open-mmlab/mmdetection)
and [mmpose](https://github.com/open-mmlab/mmpose).

![](https://raw.githubusercontent.com/hysts/anime-face-detector/main/assets/output.jpg)
(To avoid copyright issues, the above demo uses images generated by the
[TADNE](https://thisanimedoesnotexist.ai/) model.)

The model detects near-frontal anime faces and predicts 28 landmark points.
![](https://raw.githubusercontent.com/hysts/anime-face-detector/main/assets/landmarks.jpg)

The result of k-means clustering of landmarks detected in real images:
![](https://raw.githubusercontent.com/hysts/anime-face-detector/main/assets/cluster_pts.png)

The mean images of real images belonging to each cluster:
![](https://raw.githubusercontent.com/hysts/anime-face-detector/main/assets/cluster_mean.jpg)

## Requirements

- Python 3.10-3.11
- PyTorch == 2.9.1
- OpenMMLab 2.0:
  - mmengine == 0.10.7
  - mmcv == 2.1.0
  - mmdet == 3.2.0
  - mmpose == 1.3.2

## Installation

### Using uv (linux)

```bash
sudo apt-get install -y ninja-build
uv sync

mkdir -p deps && cd deps
git clone https://github.com/jin-s13/xtcocoapi.git
cd xtcocoapi && ../../.venv/bin/python -m pip install -e . && cd ../..

# nvcc --versionでcudaのバージョンを確認して適合するtorchをインストールする
# https://pytorch.org/get-started/previous-versions/
uv pip install torch==2.9.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121

uv pip install openmim mmengine
uv cache clean mmcv --force

# For other GPU architectures, adjust TORCH_CUDA_ARCH_LIST:
# - Blackwell (RTX 50XX): "12.0"
# - Hopper (H100): "9.0"
# - Ada Lovelace (RTX 40xx): "8.9"
# - Ampere (RTX 30xx, A100): "8.0,8.6"
# - Turing (RTX 20xx): "7.5"
# - Volta (V100): "7.0"
MMCV_WITH_OPS=1 FORCE_CUDA=1 TORCH_CUDA_ARCH_LIST="12.0" pip install mmcv==2.1.0 --no-cache-dir --no-build-isolation
uv pip install --no-cache-dir mmdet==3.2.0 mmpose==1.3.2
uv pip install --no-cache-dir gradio  # option
```

## Docker (GPU)

Pre-built Docker image with CUDA support is available on GitHub Container Registry.

### Pull and run

```bash
# Pull the pre-built image
docker pull ghcr.io/ayutaz/anime-face-detector:gpu-cuda12.1

# Run with GPU support
docker run --gpus all -v /path/to/your/images:/data ghcr.io/ayutaz/anime-face-detector:gpu-cuda12.1 python -c "
from anime_face_detector import create_detector
import cv2

detector = create_detector('yolov3', device='cuda:0')
image = cv2.imread('/data/your_image.jpg')
preds = detector(image)
print(f'Detected {len(preds)} faces')
"
```

### Build from source

If you need to build the Docker image yourself:

```bash
git clone https://github.com/ayutaz/anime-face-detector
cd anime-face-detector
docker build -t anime-face-detector:gpu .
```

**Note:** Building from source takes 30-60 minutes because mmcv needs to be compiled with CUDA ops.

## Usage
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hysts/anime-face-detector/blob/main/demo.ipynb)

### Basic Usage with Pre-trained Models

```python
import cv2

from anime_face_detector import create_detector

detector = create_detector('yolov3')
image = cv2.imread('assets/input.jpg')
preds = detector(image)
print(preds[0])
```

### Using Custom YOLOv8 Models (MMDetection)

You can use your own trained YOLOv8 models for anime face detection:

```python
import pathlib
import cv2

from anime_face_detector import create_detector

# Create detector with custom YOLOv8 model
detector = create_detector(
    face_detector_name='yolov8',
    custom_detector_config_path=pathlib.Path('path/to/your/yolov8_config.py'),
    custom_detector_checkpoint_path=pathlib.Path('path/to/your/yolov8_weights.pth'),
    device='cuda:0'
)

image = cv2.imread('your_image.jpg')
preds = detector(image)
```

You can also use custom configurations or checkpoints with the existing YOLOv3 or Faster-RCNN models:

```python
detector = create_detector(
    face_detector_name='yolov3',
    custom_detector_checkpoint_path=pathlib.Path('path/to/custom/yolov3_weights.pth')
)
```

### Using ADetailer Models (Ultralytics)

You can use [ADetailer](https://github.com/Bing-su/adetailer) models which are based on Ultralytics YOLOv8:

```bash
# Install ultralytics
pip install ultralytics
# or with uv
uv pip install ultralytics
```

```python
import pathlib
import cv2

from anime_face_detector import create_detector

# Download model from Hugging Face
from huggingface_hub import hf_hub_download
model_path = hf_hub_download("Bingsu/adetailer", "face_yolov8n.pt")

# Create detector with adetailer model (auto-detects .pt extension)
detector = create_detector(
    custom_detector_checkpoint_path=pathlib.Path(model_path)
)

# Or explicitly specify the framework
detector = create_detector(
    custom_detector_checkpoint_path=pathlib.Path(model_path),
    detector_framework='ultralytics'
)

image = cv2.imread('your_image.jpg')
preds = detector(image)
```

Available adetailer models:
- `face_yolov8n.pt` - Nano model (fastest)
- `face_yolov8s.pt` - Small model
- `face_yolov8m.pt` - Medium model (more accurate)

```
{'bbox': array([2.2450244e+03, 1.5940223e+03, 2.4116030e+03, 1.7458063e+03,
        9.9987185e-01], dtype=float32),
 'keypoints': array([[2.2593938e+03, 1.6680436e+03, 9.3236601e-01],
        [2.2825300e+03, 1.7051841e+03, 8.7208068e-01],
        [2.3412151e+03, 1.7281011e+03, 1.0052248e+00],
        [2.3941377e+03, 1.6825046e+03, 5.9705663e-01],
        [2.4039426e+03, 1.6541921e+03, 8.7139702e-01],
        [2.2625220e+03, 1.6330233e+03, 9.7608268e-01],
        [2.2804077e+03, 1.6408495e+03, 1.0021354e+00],
        [2.2969380e+03, 1.6494972e+03, 9.7812974e-01],
        [2.3357908e+03, 1.6453258e+03, 9.8418534e-01],
        [2.3475276e+03, 1.6355408e+03, 9.5060223e-01],
        [2.3612463e+03, 1.6262626e+03, 9.0553057e-01],
        [2.2682278e+03, 1.6631940e+03, 9.5465249e-01],
        [2.2814783e+03, 1.6616484e+03, 9.0782022e-01],
        [2.2987590e+03, 1.6692812e+03, 9.0256405e-01],
        [2.2833625e+03, 1.6879142e+03, 8.0303693e-01],
        [2.2934949e+03, 1.6909009e+03, 8.9718056e-01],
        [2.3021218e+03, 1.6863715e+03, 9.3882143e-01],
        [2.3471826e+03, 1.6636573e+03, 9.5727938e-01],
        [2.3677822e+03, 1.6540554e+03, 9.4890594e-01],
        [2.3889211e+03, 1.6611255e+03, 9.5125675e-01],
        [2.3575544e+03, 1.6800433e+03, 8.5919142e-01],
        [2.3688926e+03, 1.6800665e+03, 8.3275074e-01],
        [2.3804905e+03, 1.6761322e+03, 8.4160626e-01],
        [2.3165366e+03, 1.6947096e+03, 9.1840971e-01],
        [2.3282458e+03, 1.7104808e+03, 8.8045174e-01],
        [2.3380054e+03, 1.7114034e+03, 8.8357794e-01],
        [2.3485500e+03, 1.7080273e+03, 8.6284375e-01],
        [2.3378748e+03, 1.7118135e+03, 9.7880816e-01]], dtype=float32)}
```

### Pretrained models

[Here](https://github.com/hysts/anime-face-detector/releases/tag/v0.0.1) are the pretrained models.
(They will be automatically downloaded when you use them.)

## Demo (using [Gradio](https://github.com/gradio-app/gradio))
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange)](https://huggingface.co/spaces/ayousanz/anime-face-detector-gpu)

### Run locally
```bash
pip install gradio
git clone https://github.com/hysts/anime-face-detector
cd anime-face-detector

python demo_gradio.py
```

## Citation
If you find this repo useful for your research, please consider citing it:
```bibtex
@misc{anime-face-detector,
  author = {hysts},
  title = {Anime Face Detector},
  year = {2021},
  howpublished = {\url{https://github.com/hysts/anime-face-detector}}
}
```

## Links
### General
- https://github.com/open-mmlab/mmdetection
- https://github.com/open-mmlab/mmpose

### Anime face detection
- https://github.com/zymk9/yolov5_anime [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange)](https://huggingface.co/spaces/hysts/yolov5_anime)
- https://github.com/qhgz2013/anime-face-detector
- https://github.com/cheese-roll/light-anime-face-detector
- https://github.com/nagadomi/lbpcascade_animeface [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange)](https://huggingface.co/spaces/hysts/lbpcascade_animeface)

### Anime face landmark detection
- https://github.com/kanosawa/anime_face_landmark_detection [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-orange)](https://huggingface.co/spaces/hysts/anime_face_landmark_detection)

### Others
- https://www.gwern.net/Faces
- https://thisanimedoesnotexist.ai
